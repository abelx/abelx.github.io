---
 
 layout: post
 title: Word2vec-vs.-fastText
 subtitle: "主要对比两者的区别，写的比较简单。"
 date: 2018-03-19 
 author: abelx 
 category: NLP
 tags: 
 finished: True
 
--- 

## word2vec
Xin Rong 的论文：[『word2vec Parameter Learning Explained](https://arxiv.org/abs/1411.2738)讲的非常好，主要是讲推导，有助于理解heritical softmax和negative sampling。

### 两种模型

- 层次结构：one_hot_input->[embeding_layer]->hidden->[output_layer]->softmax/heritical_softmax/negative_sampling
- tensor的shape：v->(v, d)->d->(d, v)/(d, v-1)，其中d是word vector的size，v是词典的size
- skipgram：一个word预测上下文k个word，loss等于k个目标词的loss之和
- cbow：k个context word预测一个word，隐层等于k个word的embeding vector求mean

### heritical softmax
- 权重更新的复杂度是O(d*log(v))，用一般的softmax的话是O(d*v)
- 输出层不再是表示词典中每个word的结点，而是一个霍夫曼树，每个叶节点表示一个word。实际上就是把一个v分类问题变成了lg(v)个2分类问题，每个非叶结点都表示一个二分类问题。
参考 [1](http://www.trevorsimonton.com/blog/2016/12/15/huffman-tree-in-word2vec.html) & [2](https://zhuanlan.zhihu.com/p/26306795)

### negative sampling
- 权重更新的复杂度是O(d*k)
- 每次forward和backward都在整个词典上随机采样k个word负样本，当然只有目标word是正样本，计算softmax
- 它和heritical softmax都是加快训练的trick，减小计算量

## fastText
包含word representation和text classification两个部分

### word representation
代码的一些细节可以参考博文[3](https://heleifz.github.io/14732610572844.html)
- 和word2vec有两个模型，两个trick
- 可以表示OOV(out of vocabulary) word，用到了char n-gram方法，每个char gram都有一个向量，用所有gram的和表示单词，参考论文[4](https://arxiv.org/abs/1607.04606)

### text classification
- 模型和skipgram很像，在输入层增加了word n-gram，隐层是输入层经过embeding后的mean。层次结构：one_hot_input->[embeding_layer]->hidden->[output_layer]->softmax/heritical_softmax/negative_sampling
- tensor的shape：v+b->(v+b, d)->d->(d, c)/(d, v-1)，其中d是word vector的size，v是词典的size，b是hash表的buket数，c是类别数
- 使用了hash方法来保存n-gram
